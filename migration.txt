# torch.arange()                                   => tf.constant(tf.experimental.numpy.arange())
# torch.autograd.Variable()                        => tf.Variable()
# torch.cat()                                      => tf.concat()
# torch.cuda.manual_seed_all()                     => NOT FOUND; USE tf.random.set_seed() INSTEAD!!
# torch.from_numpy()                               => tf.convert_to_tensor()
# torch.load()                                     => tf.saved_model.load()
# torch.manual_seed()                              => tf.random.set_seed()
# torch.max()                                      => tf.math.reduce_max()
# torch.nn._Loss()                                 => tf.keras.losses.Loss()
# torch.nn.BatchNorm1d()                           => tf.keras.layers.BatchNormalization()
        -> num_features=                           => NOT FOUND (NOT axis)
# torch.nn.BCELoss()                               => tf.keras.losses.BinaryCrossentropy()
# torch.nn.Conv1d()                                => tf.keras.layers.Conv1D() (MISSING ARGUMENT FOR INPUT CHANNELS)
# torch.nn.DataParallel()                          => REMOVED AND REPLACED BY tf.Module
# torch.nn.Dropout()                               => tf.keras.layers.Dropout()
        -> p=                                      -> rate= (PRESUMABLY)
# torch.nn.Linear()                                => tf.keras.layers.ELU (MAYBE) (FAR: tfl.layers.Linear())
# torch.nn.MaxPool1d()                             => tf.keras.layers.MaxPool1D()
# torch.nn.Module()                                => tf.Module()
        -> .modules()                              => .submodules
        -> .parameters()                           => .variables
        -> .state_dict()                           =>
# torch.nn.ReLU()                                  => tf.keras.layers.ReLU()
# torch.nn.Sequential()                            => tf.keras.Sequential()
# torch.nn.Sigmoid()                               => NOT FOUND!!
# torch.no_grad()                                  => INSIDE tf.Variable() SET trainable=False
# torch.optim.Optimizer()                          => tf.keras.optimizers.Optimizer()
        -> .weight_decay                           =>
# torch.optim.lr_scheduler.ReduceLROnPlateau()     =>
# torch.optim.SGD()                                => tf.keras.optimizers.SGD()
# torch.save()                                     => tf.saved_model.save()
# torch.set_num_threads()                          => tf.config.threading.set_intra_op_parallelism_threads()
# torch.squeeze()                                  => tf.squeeze()
# torch.Tensor()                                   => tf.constant() (-> tf.Tensor())
        -> [tf.Tensor]                             => tf.gather(y, ya) (CERTAIN)
        -> [:, tf.Tensor]                          => tf.gather(y, ya, axis=tf.constant(1)) (CERTAIN)
        -> .contiguous()                           => PRESUMABLY NOTHING, IGNORE IT
        -> .dim()                                  => tf.Tensor().ndim
        -> .is_cuda                                => "cuda" in x.device.lower() (irrelevant: tf.test.is_built_with_cuda() also tf.test.is_gpu_available())
        -> .long()                                 => tf.cast(tf.Tensor, tf.int64)
        -> .size()                                 => tf.Tensor().shape
        -> .size(dim: int)                         => tf.Tensor().shape[dim]
        -> .sum()                                  => tf.size(tf.Tensor) (ALMOST CERTAINLY)
        -> .transpose()                            => tf.transpose(tf.Tensor)
        -> .unsqueeze_()                           => tf.expand_dims(tf.Tensor) (PRESUMABLY)
        -> .view(d...)                             => tf.reshape(tf.Tensor, [d...]) (ALMOST CERTAINLY)
# torch.utils.data.DataLoader()                    =>
# torch.utils.data.Dataset()                       => tf.data.Dataset()
# torch.utils.data.sampler.SubsetRandomSampler()   =>
