# torch.arange()                                   => tf.constant(tf.experimental.numpy.arange())
# torch.autograd.Variable()                        => tf.Variable()
# torch.cat()                                      => tf.concat()
# torch.cuda.manual_seed_all()                     =>
# torch.from_numpy()                               => tf.convert_to_tensor()
# torch.load()                                     =>
# torch.manual_seed()                              =>
# torch.max()                                      => tf.math.reduce_max()
# torch.nn._Loss()                                 =>
# torch.nn.BatchNorm1d()                           =>
# torch.nn.BCELoss()                               =>
# torch.nn.Conv1d()                                =>
# torch.nn.DataParallel()                          => REMOVED AND REPLACED BY tf.Module
# torch.nn.Dropout()                               =>
# torch.nn.Linear()                                =>
# torch.nn.MaxPool1d()                             =>
# torch.nn.Module()                                => tf.Module (BADLY REPLACED!!)
# torch.nn.ReLU()                                  =>
# torch.nn.Sequential()                            =>
# torch.nn.Sigmoid()                               =>
# torch.no_grad()                                  => INSIDE tf.Variable() SET trainable=False
# torch.optim.Optimizer()                          => tf.keras.optimizers.Optimizer() (MAYBE!!)
# torch.optim.lr_scheduler.ReduceLROnPlateau()     =>
# torch.optim.SGD                                  =>
# torch.save()                                     =>
# torch.set_num_threads()                          => tf.config.threading.set_intra_op_parallelism_threads()
# torch.squeeze()                                  => tf.squeeze()
# torch.Tensor()                                   => tf.constant() (-> tf.Tensor())
        -> [tf.Tensor]                             => tf.gather(y, ya) (CERTAIN)
        -> [:, tf.Tensor]                          => tf.gather(y, ya, axis=tf.constant(1)) (CERTAIN)
        -> .contiguous()                           => PRESUMABLY NOTHING, IGNORE IT!
        -> .dim()                                  => tf.Tensor().ndim
        -> .is_cuda                                => "cuda" in x.device.lower() (irrelevant: tf.test.is_built_with_cuda() also tf.test.is_gpu_available())
        -> .long()                                 => tf.cast(tf.Tensor, tf.int64)
        -> .size()                                 => tf.Tensor().shape
        -> .size(dim: int)                         => tf.Tensor().shape[dim]
        -> .sum()                                  => tf.size(tf.Tensor) (ALMOST CERTAINLY!!)
        -> .transpose()                            => tf.transpose(tf.Tensor)
        -> .unsqueeze_()                           => tf.expand_dims(tf.Tensor) (PRESUMABLY!!)
        -> .view(d...)                             => tf.reshape(tf.Tensor, [d...]) (ALMOST CERTAINLY!!)
# torch.utils.data.DataLoader()                    =>
# torch.utils.data.Dataset()                       =>
# torch.utils.data.sampler.SubsetRandomSampler()   =>
